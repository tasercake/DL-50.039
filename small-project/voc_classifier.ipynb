{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Resnet50-based model with sigmoid outputs.\n",
    "\n",
    "No learning rate scheduler.\n",
    "\n",
    "Converges in ~10 epochs on PascalVOC training set.\n",
    "\n",
    "#### Hyperparameters\n",
    "- Learning Rate: `1e-4`\n",
    "- Optimizer: `Adam`\n",
    "- Batch size: `8`\n",
    "- Loss function: *Classwise Binary Cross-Entropy*\n",
    "\n",
    "#### Metrics\n",
    "- Accuracy: `0.9628`\n",
    "- Mean AP: `0.7286`\n",
    "- Mean Tail Accuracy: `0.9116`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Install this project's dependencies via `pip install -r requirements.txt`\n",
    "\n",
    "I use [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/introduction_guide.html) to facilitate experiment logging. This library does not abstract any significant functionality of Pytorch other than the training loop (which is mostly copy-pasted code anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/miniconda3/envs/ai/lib/python3.7/site-packages/pytorch_lightning/core/decorators.py:13: UserWarning: data_loader decorator deprecated in 0.6.1. Will remove 0.8.0\n",
      "  warnings.warn(w)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from voc_data import VOCDataset, PascalVOC\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel Classifier\n",
    "\n",
    "The classifier is based on Resnet50 (pre-trained on ImageNet).\n",
    "\n",
    "The final fully-connected layer is replaced with a new randomly initialized layer that has 20 outputs rather than the default 1000, and the sigmoid function is applied to its outputs.\n",
    "\n",
    "## Model Training\n",
    "\n",
    "The loss function I use is based on Pytorch's [`binary_cross_entropy`](https://pytorch.org/docs/stable/nn.functional.html#binary-cross-entropy). This function is applied class-wise and averaged to obtain the loss value (see `VOCClassifier._multi_label_loss` for implementation).\n",
    "\n",
    "The label for each sample is represented as a many-hot vector, with negative classes being `0` and positive ones being `1`.\n",
    "\n",
    "During both training and inference, images are resized (shortest edge = 400px), FiveCropped and stacked, then normalized.\n",
    "\n",
    "During training, I apply additional augmentation (random horizontal flipping and rotation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCClassifier(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-4, log_every_n_steps=10):\n",
    "        super().__init__()\n",
    "        self.log_every_n_steps = log_every_n_steps\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Model definition\n",
    "        self.stem = torchvision.models.resnet50(pretrained=True, progress=True)\n",
    "        self.stem.fc = torch.nn.Linear(2048, 20)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 5:\n",
    "            bs, ncrops, c, h, w = x.size()\n",
    "            x = self.stem(x.view(-1, c, h, w))\n",
    "            x = x.view(bs, ncrops, -1).max(1)[0]\n",
    "        elif len(x.shape) == 4:\n",
    "            x = self.stem(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Expected input to have rank 4 or 5, got {x.shape} (rank {len(x.shape)}) instead\")\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _multi_label_loss(pred, labels):\n",
    "        loss = torch.stack([F.binary_cross_entropy(\n",
    "            pred[:, i],\n",
    "            labels[:, i]\n",
    "        ) for i in range(labels.shape[1])]).mean()\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        image, labels, _ = batch\n",
    "        pred = self.forward(image)\n",
    "#         loss = F.binary_cross_entropy(pred, labels)\n",
    "        loss = self._multi_label_loss(pred, labels)\n",
    "        output = {\"loss\": loss}\n",
    "        if self.log_every_n_steps and batch_idx % self.log_every_n_steps == 0:\n",
    "            tensorboard_logs = {\"train_loss\": loss}\n",
    "            output[\"log\"] = tensorboard_logs\n",
    "        return output\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        image, labels, _ = batch\n",
    "        pred = self.forward(image)\n",
    "#         loss = F.binary_cross_entropy(pred, labels)\n",
    "        loss = self._multi_label_loss(pred, labels)\n",
    "        correct = ((pred > 0.5) == labels).sum().float()\n",
    "        count = labels.shape[0] * labels.shape[1]\n",
    "        output = {\n",
    "            \"val_loss\": loss,\n",
    "            \"correct\": correct, \"count\": count,\n",
    "            \"pred\": pred, \"labels\": labels\n",
    "        }\n",
    "        return output\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        correct = torch.stack([x[\"correct\"] for x in outputs]).sum()\n",
    "        accuracy = correct / sum([x[\"count\"] for x in outputs])\n",
    "        all_labels = torch.cat([x[\"labels\"] for x in outputs]).cpu().detach().numpy()\n",
    "        all_pred = torch.cat([x[\"pred\"] for x in outputs]).cpu().detach().numpy()\n",
    "        mean_ap = average_precision_score(all_labels, all_pred, None).mean()\n",
    "\n",
    "        tensorboard_logs = {\"val_loss\": avg_val_loss, \"val_acc\": accuracy}\n",
    "        if np.isfinite(mean_ap):\n",
    "            tensorboard_logs[\"mean_ap\"] = mean_ap\n",
    "        return {\"val_loss\": avg_val_loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"./data/VOCdevkit/VOC2012/\"\n",
    "MODEL_PATH = Path(\"models/model_fivecrop_flip.pt\")\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "SHUFFLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC data helper\n",
    "voc = PascalVOC(ROOT_DIR)\n",
    "\n",
    "# Data transforms\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "to_tensor = transforms.ToTensor()\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize(400),\n",
    "    transforms.FiveCrop(224),\n",
    "    transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(crop)) for crop in crops])),\n",
    "])\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(25),\n",
    "    base_transform,\n",
    "])\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "train_dataset = VOCDataset(voc, split=\"train\", transform=train_transform)\n",
    "val_dataset = VOCDataset(voc, split=\"val\", transform=base_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCClassifier(\n",
      "  (stem): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=2048, out_features=20, bias=True)\n",
      "  )\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = VOCClassifier(learning_rate=LEARNING_RATE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pytorch_lightning` comes in here.\n",
    "\n",
    "It handles the training loop and the moving of tensors to GPU(s), as well as logging of metrics for visualization in Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl_logger = pl.loggers.TestTubeLogger(save_dir=\"experiments/\")\n",
    "trainer = pl.Trainer(\n",
    "    gpus=[0],\n",
    "    logger=pl_logger,\n",
    "    progress_bar_refresh_rate=10,\n",
    "#     val_check_interval=0.1,\n",
    "#     val_percent_check=0.25,\n",
    "    overfit_pct=0.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validation sanity check', layout=Layout(flex='2'), max=5.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/miniconda3/envs/ai/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780bfb1777c74b14a401d1710f27d5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=1.0), HTML(value='')), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=182.0, style=Pr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/miniconda3/envs/ai/lib/python3.7/site-packages/pytorch_lightning/trainer/training_io.py:244: UserWarning: Did not find hyperparameters at model.hparams. Saving checkpoint without hyperparameters\n",
      "  \"Did not find hyperparameters at model.hparams. Saving checkpoint without\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=182.0, style=Pr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/miniconda3/envs/ai/lib/python3.7/site-packages/pytorch_lightning/trainer/training_io.py:244: UserWarning: Did not find hyperparameters at model.hparams. Saving checkpoint without hyperparameters\n",
      "  \"Did not find hyperparameters at model.hparams. Saving checkpoint without\"\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_PATH.exists():\n",
    "    raise ValueError(\"Model with that name already exists. Refusing to overwrite.\")\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = torch.load(MODEL_PATH)\n",
    "model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, cuda=True, subset=None):\n",
    "    all_labels = []\n",
    "    all_pred = []\n",
    "    all_paths = []\n",
    "\n",
    "    if subset:\n",
    "        assert isinstance(subset, float) and 0 < subset < 1\n",
    "    max_samples = int(np.ceil(len(dataloader) * subset)) if subset else len(dataloader)\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        for i, (image, labels, path) in tqdm(enumerate(dataloader), total=max_samples, ncols='100%'):\n",
    "            if cuda:\n",
    "                model.cuda()\n",
    "                image = image.cuda()\n",
    "                labels = labels.cuda()\n",
    "            pred = model(image)\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_pred.append(pred.cpu().numpy())\n",
    "            all_paths.append(path)\n",
    "\n",
    "            if subset and i >= max_samples:\n",
    "                break\n",
    "\n",
    "    labels = np.concatenate(all_labels, 0)\n",
    "    pred = np.concatenate(all_pred, 0)\n",
    "    paths = np.concatenate(all_paths)\n",
    "\n",
    "    return pred, labels, paths\n",
    "\n",
    "pred, labels, paths = get_predictions(model, val_dataloader, cuda=True, subset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(image_paths, title, size=200):\n",
    "    num_images = len(image_paths)\n",
    "    images = [Image.open(path) for path in image_paths]\n",
    "    fig, axs = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "\n",
    "    for i, ax in enumerate(axs.reshape(-1)):\n",
    "        ax.axis(\"off\")\n",
    "        ax.imshow(Image.open(image_paths[i]))\n",
    "    fig.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_top_and_bottom_k(\n",
    "    pred, paths, cat_names,\n",
    "    k=50, num_classes=5, total_classes=20,\n",
    "    output_dir=\"data/output\",\n",
    "):\n",
    "    # Top and bottom K image visualization (on randomly chosen classes)\n",
    "    output_path = Path(output_dir)\n",
    "    class_indices = np.random.choice(list(range(total_classes)), size=num_classes, replace=False)\n",
    "    for class_index in class_indices:\n",
    "        class_name = cat_names[class_index]\n",
    "        sorted_indices = np.argsort(pred[:, class_index])\n",
    "        # Top K\n",
    "        top = sorted_indices[-k:]\n",
    "        for i, src in enumerate(paths[top]):\n",
    "            dst = output_path / f\"top/{class_name}/{k - i}.jpg\"\n",
    "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(src, dst)\n",
    "        show_images(paths[top], f\"{class_name}: Top {k}\")\n",
    "        # Bottom K\n",
    "        bottom = sorted_indices[:k]\n",
    "        for i, src in enumerate(paths[bottom]):\n",
    "            dst = output_path / f\"bottom/{class_name}/{i + 1}.jpg\"\n",
    "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(src, dst)\n",
    "        show_images(paths[bottom], f\"{class_name}: Bottom {k}\")\n",
    "\n",
    "def show_tail_accuracy(pred, labels, start, num_steps=20):\n",
    "    end = pred.max(0)[0].min()\n",
    "    steps = np.linspace(start, end, num=num_steps)[:-1]\n",
    "\n",
    "    tail_accuracies = []\n",
    "    for step in steps:\n",
    "        classwise_tailacc = np.reshape(\n",
    "            (np.sum((pred > step) * labels, 0) / (np.sum(pred > step, 0) + 1e-5)),\n",
    "            (1, -1)\n",
    "        )\n",
    "        tail_accuracies.append(classwise_tailacc)\n",
    "    tail_accuracies = np.concatenate(tail_accuracies)\n",
    "    print(\"Tail accuracies\", tail_accuracies[-1])\n",
    "    print(\"Mean tail accuracy\", tail_accuracies[-1].mean())\n",
    "    plt.figure()\n",
    "    plt.plot(steps, tail_accuracies.mean(1))\n",
    "    plt.title(\"Tail Accuracy\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Tail Accuracy\")\n",
    "    plt.savefig(\"tail_accuracy.png\")\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(pred, labels, paths, voc):\n",
    "    cat_names = voc.list_image_sets()\n",
    "\n",
    "    # Binarize predictions\n",
    "    pred_binary = pred > 0.5\n",
    "\n",
    "    # Average precision\n",
    "    ap = average_precision_score(labels, pred, None)\n",
    "    mean_ap = ap.mean()\n",
    "\n",
    "    # Accuracy\n",
    "    correct = np.sum(pred_binary == labels)\n",
    "    total = labels.shape[0] * labels.shape[1]\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Average Precision:\", ap)\n",
    "    print(\"Mean Average Precision:\", mean_ap)\n",
    "    visualize_top_and_bottom_k(\n",
    "        pred, paths, cat_names,\n",
    "        k=5, num_classes=5, total_classes=20,\n",
    "        output_dir=\"data/output\",\n",
    "    )\n",
    "    show_tail_accuracy(pred, labels, 0, num_steps=20)\n",
    "\n",
    "evaluate_model(pred, labels, paths, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "for image, labels in train_dataloader:\n",
    "    pred = model(image.cuda()[:1])\n",
    "    print(\"Ground Truth:\", labels[:1])\n",
    "    print(\"Predictions:\", pred)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
